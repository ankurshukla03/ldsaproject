\section{Computational Experiment}

%TODO: The main section of the report. Describe and motivate the choice of tools and the distributed system that you designed. Describe how you have designed your scalability experiments,and present the results.

This section provides the motivation for our choice of tools and briefly describes the distributed system that we designed. 

\subsection{Tools \& Implementations}
We are using Spark as it provides a new model of computation that allows to perform parallel operations on a specific set of data. In our project we apply multiple functions repeatedly to the same data set to extract various statistical information. Spark offers the possibility to improve performance significantly without reloading the data form disk each time \cite{Sparkzaharia}.The feature which supports much faster processing is in-memory computation.

Our dataset is read by collection of objects called RDD, these objects allow parallel operations with an option to cache an RDD in memory, so that it could be reused for other MapReduce operations. 

We are constructing RDD from a \textit{.csv} file using shared file system. The System we have is fault tolerant, in fact if a partition of an RDD is lost it is possible to rebuild just that partition given the information on how it was derived from other RDDs. This is called \textit{lineage}\cite{Sparkzaharia}.

In our system a \textit{driver program} connects to a cluster of \textit{workers}. These are processes that can store RDD partitions in RAM across operations\cite{RDD}.\newline
Our final code used pyspark RDD and dataframe for analyzing the dataset. We implemented a cluster with single worker node and tested it; so that we could abstract the data we wanted. When the implemented configuration was working correctly, we get to implement a cluster with a master node and 3 worker nodes.When we ran our code on 35 Gb dataset, it divides the data into 288 partitions and observed that the computation on 3 worker node consumed lesser time over the single worker node configuration; the observed values are represented in Figure \ref{fig:stages_three_fourth}. 

We created Virtual Machine at SNIC Science Cloud \cite{snic} and used the following settings:

\begin{itemize}
    \item \textbf{Source:} Volume Snapshot of 'snapshotgroup12'. We used the snapshot volume from SNIC to configure Spark environment.
    \item \textbf{Flavor:} $ssc.medium$ which had 2 VCPUs, 4Gb Ram, 40 Gb total disk and root disk
    \item \textbf{Networks:} SNIC 2018/10-13 Internal IPv4 Network.
    \item Our Cluster with 3 worker node has total of 6 cores(2 cores each) with the memory of 8.6 Gb(2.9 Gb each). You can see our Spark UI with 3 worker node in Figure \ref{fig:spark_master_three}. 
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=.95\linewidth]{figures/spark_master_three.jpg}
    \caption{Screen-shot of our Spark UI Cluster with 3 worker nodes.}
    \label{fig:spark_master_three}
\end{figure}


\subsection{Results}
For this project, we used Chicago city Crimes data set\cite{cityOfChicago} and it contained many relevant information associated with each of the crime that has occurred in Chicago from year 2001 until now. We are plotting 4 different graphs associated with 4 different columns. A program in pyspark was created to plot these graph. You can look at the code \href{https://github.com/ankurshukla03/ldsaproject/blob/edit_report/code/Crimes.ipynb}{here.} \\
Below is 4 different plots presenting the results :\\

\mfigure{crime_type}{Plot representing the frequency of particular crime type that has occurred in Chicago from year 2001 until now. Our  generated plots seemed similar to the Figure \ref{fig:exampleGraph} which we took from Chicago portal \cite{cityOfChicago}.}

\mfigure{month}{Plot representing the frequency of crime across the months.}

\mfigure{year}{Plot representing the frequency of crime per year.}

\mfigure{location}{Plot representing the frequency of crime with location description. Our plot is correct when compared with the plot in the dashboard of Chicago portal \href{https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present-Dashboard/5cd6-ry5g}{here.}}


\subsection{Scalability experiment}
In order to test and confirm the scalability of the system, scalability studies were conducted. Since the original dataset was only of 1,45 GB, it was scaled up by replicating it in order to get larger datasets. Four different sizes of the dataset were produced including the original one. These were 1.45 Gb, 11.2 Gb, 23.2 Gb and 35 Gb and you can see the time taken with different number of nodes on these data size in Table \ref{table_time}.

To measure the horizontal scalability, the run time for a specific job on the dataset were recorded for 1 and 3 nodes cluster configuration. As seen in  \ref{fig:scalabilityGraph} below we can observe that
the run time increased linearly with data size for the different cluster configurations and the run time was reduced equally for increasing number of nodes. Furthermore, the slopes for the linear fits of the data points are shown. The slope for one node is 41.241 s/GB and for four nodes it is 13.114 s/GB. This means that the second configuration is 3,15 times faster.\\

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|}
\hline
Number of Nodes  & Time Taken & Data Size \\ \hline
        1 & 1.1 min & 1.45 Gb \\ \hline
        3 & 32 seconds & 1.45 Gb \\
\hline
\end{tabular}
\quad
\begin{tabular}{|c|c|c|}
\hline
Number of Nodes  & Time Taken & Data Size \\ \hline
        1 & 4.1 min & 5.8 Gb \\ \hline
        3 & 1.4 min & 5.8 Gb \\
\hline
\end{tabular}
\begin{tabular}{|c|c|c|}
\hline
Number of Nodes  & Time Taken & Data Size \\ \hline
        1 & 16 min & 23.2 Gb \\ \hline
        3 & 5.1 min & 23.2 Gb \\
\hline
\end{tabular}
\quad
\begin{tabular}{|c|c|c|}
\hline
Number of Nodes  & Time Taken & Data Size \\ \hline
        1 & 24 min & 35 Gb \\ \hline
        3 & 7.6 min & 35 Gb \\
\hline
\end{tabular}
    \caption{Time taken by reduce functionality using different cluster configuration over different data size.}
    \label{table_time}
\end{table}


\mfigure{stages_one_fourth}{Screenshot of our Spark UI stages tab when ran with 1 node on 35 Gb data size.}

\mfigure{stages_three_second}{Screenshot of our Spark UI stages tab when ran with 3 node on 5.8 Gb data size.}

\mfigure{stages_three_fourth}{Screenshot of our Spark UI stages tab when ran with 3 worker node on 35 Gb data size.}


\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{figures/runTime2.PNG}
    \caption{Run time for different data sizes plotted for two cluster configurations. Blue line denotes the cluster with one node and the red line denotes with three nodes.}
    \label{fig:scalabilityGraph}
\end{figure}

